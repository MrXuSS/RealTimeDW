# 创新应用研究中心学习计划（项目篇）

版本：V20200403

## 1、离线计算

要求：使用 Spark SQL 完成程序开发并提交至集群运行

### 1.1 数据分析

数据说明：

```properties
（1）档案数据（ArchiveData）结构[UserID, DeviceID]
	案例数据：
	U0000001,D0000001
	U0000001,D0000002
	U0000002,D0000003
（2）设备数据（DeviceData）结构[DeviceID, DataTime, DeviceData]
	案例数据：
	D0000001,2020040201,220
	D0000001,2020040202,220
	D0000001,2020040203,230
	D0000001,2020040204,220
	D0000001,2020040205,230
	D0000001,2020040206,230
	D0000001,2020040207,270
	D0000001,2020040208,270
	D0000001,2020040209,270
	D0000001,2020040210,270
	D0000001,2020040211,220
	D0000001,2020040212,
	,,
	D0000001,2020040214,290
	D0000001,2020040215,220
	D0000002,2020040201,220
	D0000002,2020040202,220
```

需求说明：

```verilog
1. 设备DeviceData > 220即异常，异常持续时间 >= 3小时，则输出异常信息（AbnormalInfomation）报警
（1）时间计算：2020040202 - 2020040201 = 1小时；
（2）过滤异常数据，丢失数据按照正常计算；
2. 输出异常信息结构
	[UserID,DeviceID,AbnormalStartTime,AbnormalStopTime,AbnormalDurationTime]
	AbnormalStartTime：异常开始时间
	AbnormalStopTime：异常结束时间
	AbnormalDurationTime：异常持续时间
3. 异常信息输出至HDFS保存
```

结果预期：

```properties
Tmp:
D0000001,2020040204,220>>AbnormalInfomation(U0000001,D0000001,,,0)
D0000001,2020040205,230>>AbnormalInfomation(U0000001,D0000001,2020040205,,0)
D0000001,2020040206,230>>AbnormalInfomation(U0000001,D0000001,2020040205,,1)
D0000001,2020040207,270>>AbnormalInfomation(U0000001,D0000001,2020040205,,2)
D0000001,2020040208,270>>AbnormalInfomation(U0000001,D0000001,2020040205,,3)>>out
D0000001,2020040209,270>>AbnormalInfomation(U0000001,D0000001,2020040205,,4)>>out
D0000001,2020040210,270>>AbnormalInfomation(U0000001,D0000001,2020040205,,5)>>out
D0000001,2020040211,220>>AbnormalInfomation(U0000001,D0000001,2020040205,2020040210,5)>>out

最终输出结果(保存至HDFS)：
+--------+-----------------+----------------+--------------------+--------+
|deviceID|abnormalStartTime|abnormalStopTime|abnormalDurationTime|  userID|
+--------+-----------------+----------------+--------------------+--------+
|D0000001|       2020040205|                |                   3|U0000001|
|D0000001|       2020040205|                |                   4|U0000001|
|D0000001|       2020040205|                |                   5|U0000001|
|D0000001|       2020040205|      2020040210|                   5|U0000001|
+--------+-----------------+----------------+--------------------+--------+
```

**SparkSql用的不多，不是很会，使用Hql查询**

1. **将异常信息筛选出来**
**select**
    **d.DeviceID DeviceID,**
    **d.DataTime DataTime,**
    **d.DeviceData DeviceData**
**from**
    **DeviceData d**
**where**
    **d.DeviceDate > 220;t1**

**2.对设备数据进行分组**
**select**
  **t2.DeviceID,**
  **min(t1.DataTime) start,**
  **max(t1.DataTime) stop,**
  **stop-start  DurationTime**
**from**
  **t1**
**group by**
  **t1.DeviceID;t2**

**3.对数据进行刷选，将时间持续时间大于三个小时筛选出**

**select**
  **t2.DeviceID DeviceID,**
  **t2.start start,**
  **t2.stop stop,**
  **t2.DurationTime DurationTime**
**from**
  **t2**
**where t2.DurationTime > 3;t3**

**4.将设备与用户进行关联**
**select**
  **t3.DeviceID DeviceID,**
  **t3.start start,**
  **t3.stop stop,** 
  **t3.DurationTime DurationTime,**
  **a.UserID UserID**
**from**
  **t3**
**join**
  **ArchiveData a**
**on**
  **t3.DeviceID = a.DeviceID;**

**最终：**
**select**
  **t3.DeviceID DeviceID,**
  **t3.start start,**
  **t3.stop stop,** 
  **t3.DurationTime DurationTime,**
  **a.UserID UserID**
**from**
  **t3(select**
  **t2.DeviceID DeviceID,**
  **t2.start start,**
  **t2.stop stop,**
  **t2.DurationTime DurationTime**
**from**
  **t2(select**
  **t2.DeviceID,**
  **min(t1.DataTime) start,**
  **max(t1.DataTime) stop,**
  **stop-start  DurationTime**
**from**
  **t1(select**
  **d.DeviceID DeviceID,**
  **d.DataTime DataTime,**
  **d.DeviceData DeviceData**
**from**
  **DeviceData d**
**where**
  **d.DeviceDate > 220)**
**group by**
  **t1.DeviceID)**
**where t2.DurationTime > 3)**
**join**
  **ArchiveData a**
**on**
  **t3.DeviceID = a.DeviceID;**

### 1.2 数据分析2

修改需求说明：

```verilog
1. 异常等级
（1）DeviceData > 280，判定为A级异常;
（2）260 < DeviceData <= 280，判定为B级异常;
（3）220 < DeviceData <= 260，判定为C级异常;
（4）否则正常
（5）过滤异常数据，丢失数据按照正常计算
2. 异常持续时间
（1）A级异常持续时间 >= 1小时，则输出异常信息
（2）B级异常持续时间 >= 2小时，则输出异常信息
（3）C级异常持续时间 >= 3小时，则输出异常信息
3. 异常信息结构
[UserID,DeviceID,Level,AbnormalStartTime,AbnormalStopTime,AbnormalDurationTime]
4. 异常信息输出至HDFS保存
```

结果预期：

```properties
Tmp:
D0000001,2020040204,220>>AbnormalInfomation(U0000001,D0000001,,,,0)
D0000001,2020040205,230>>AbnormalInfomation(U0000001,D0000001,C,2020040205,,0)
D0000001,2020040206,230>>AbnormalInfomation(U0000001,D0000001,C,2020040205,,1)
D0000001,2020040207,270>>AbnormalInfomation(U0000001,D0000001,B,2020040207,,0)
D0000001,2020040208,270>>AbnormalInfomation(U0000001,D0000001,B,2020040207,,1)
D0000001,2020040209,270>>AbnormalInfomation(U0000001,D0000001,B,2020040207,,2)>>out
D0000001,2020040210,270>>AbnormalInfomation(U0000001,D0000001,B,2020040207,,3)>>out
D0000001,2020040211,220>>AbnormalInfomation(U0000001,D0000001,B,2020040207,2020040210,3)>>out

最终输出结果(保存至HDFS)：
+--------+-----+-----------------+----------------+--------------------+--------+
|deviceID|level|abnormalStartTime|abnormalStopTime|abnormalDurationTime|  userID|
+--------+-----+-----------------+----------------+--------------------+--------+
|D0000001|    B|       2020040207|                |                   2|U0000001|
|D0000001|    B|       2020040207|                |                   3|U0000001|
|D0000001|    B|       2020040207|      2020040210|                   3|U0000001|
+--------+-----+-----------------+----------------+--------------------+--------+
```

Hql：

1. **将异常信息筛选出来**
**select**
    **d.DeviceID DeviceID,**
    **d.DataTime DataTime,**
    **d.DeviceData DeviceData**
**from**
    **DeviceData d**
**where**
    **d.DeviceDate > 220;t1**

**2.对异常数据进行区分等级**

**select**
  **t1.DeviceID DeviceID,**
  **t1.DataTime DataTime,**
  **t1.DeviceData DeviceData,**
  **case when DeviceData > 280 then 'A'**
       **when DeviceData > 260 and DeviceData <= 280 then 'B'**
       **when DeviceData > 220 and DeviceData <= 260 then 'C' as level**
**from**
  **t1;t2**

**3.按照设备和异常等级进行分组，就可以得到每一个设备各级异常的时间**
**select**
  **t2.DeviceID DeviceID,**
  **min(t2.DataTime) Start,**
  **max(t2.DataTime) Stop,**
  **stop-start DurationTime,**
  **t2.level level**
**from**
  **t2**
**group by**
  **t2.DeviceID, t2.level;t3**

**4.筛选出各级异常对应时间的数据**

**select**
  **t3.DeviceID DeviceID,**
  **t3.Start Start,**
  **t3.Stop Stop,**
  **t3.DurationTime DurationTime,**
  **t3.level level**
**from**
  **t3**
**where**
  **t3.level = 'A' and t3.DurationTime >= 1**
  **or**
  **t3.level = 'B' and t3.DurationTime >= 2**
  **or**
  **t3.level = 'C' and t3.DurationTime >= 3;t4**

**5.关联用户表**
**select**
  **t4.DeviceID DeviceID,**
  **t4.level level**
  **t4.Start Start,**
  **t4.Stop Stop,**
  **t4.DurationTime DurationTime,**
  **a.UserID UserID**
**from**
  **t4**
**join**
  **ArchiveData a**
**on**
  **t4.DeviceID = a.DeviceID;**

**最终：**
**select**
  **t4.DeviceID DeviceID,**
  **t4.level level**
  **t4.Start Start,**
  **t4.Stop Stop,**
  **t4.DurationTime DurationTime,**
  **a.UserID UserID**
**from**
  **t4(select**
  **t3.DeviceID DeviceID,**
  **t3.Start Start,**
  **t3.Stop Stop,**
  **t3.DurationTime DurationTime,**
  **t3.level level**
**from**
  **t3(select**
  **t2.DeviceID DeviceID,**
  **min(t2.DataTime) Start,**
  **max(t2.DataTime) Stop,**
  **stop-start DurationTime,**
  **t2.level level**
**from**
  **t2(select**
  **t1.DeviceID DeviceID,**
  **t1.DataTime DataTime,**
  **t1.DeviceData DeviceData,**
  **case when DeviceData > 280 then 'A'**
       **when DeviceData > 260 and DeviceData <= 280 then 'B'**
       **when DeviceData > 220 and DeviceData <= 260 then 'C' as level**
**from**
  **t1(select**
  **d.DeviceID DeviceID,**
  **d.DataTime DataTime,**
  **d.DeviceData DeviceData**
**from**
  **DeviceData d**
**where**
  **d.DeviceDate > 220))**
**group by**
  **t2.DeviceID, t2.level)**
**where**
  **t3.level = 'A' and t3.DurationTime >= 1**
  **or**
  **t3.level = 'B' and t3.DurationTime >= 2**
  **or**
  **t3.level = 'C' and t3.DurationTime >= 3)**
**join**
  **ArchiveData a**
**on**
  **t4.DeviceID = a.DeviceID;**

### 1.3 考察点

```properties
1. DataFrame创建
2. 数据清洗
3. 数据聚合
4. 数据迭代
5. Spark读写HDFS
6. Spark程序打包
7. Spark任务提交
```



## 2、项目部署

### 2.1 数据源

```properties
Archive Data >> <位于hdfs> /hdfs/demo/archive/archive.txt
Device Data >>  <位于linux> /linux/demo/device/device01.txt，/app/demo/device02.txt
```

### 2.2 要求

```properties
1、使用 Flume 将 deviceXX.txt 上传至HDFS
2、定时每10分钟执行一次离线计算，（archive.txt/deviceXX.txt----Spark---->HDFS）
```

2.3 考察点

```properties
1. linux基本运维
2. Flume
```

